#!/usr/bin/env python3

# Copyright © 2012-2015 Thomas TEMPÉ <thomas.tempe@alysse.org>
# Copyright © 2019 Joseph Lorimer <joseph@lorimer.me>
#
# This file is part of Chinese Support Redux.
#
# Chinese Support Redux is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by the Free
# Software Foundation, either version 3 of the License, or (at your option) any
# later version.
#
# Chinese Support Redux is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
# FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
# more details.
#
# You should have received a copy of the GNU General Public License along with
# Chinese Support Redux.  If not, see <https://www.gnu.org/licenses/>.

from argparse import ArgumentParser
from functools import reduce
from os import remove
from os.path import join
from re import finditer, IGNORECASE, match, sub
from sqlite3 import IntegrityError, connect
from tempfile import TemporaryFile
from urllib.request import urlopen
from zipfile import ZipFile

from yaspin import yaspin

DATA_DIR = 'data'
DB_PATH = 'chinese_dict.sqlite'
LICENSE_PATH = 'COPYING.txt'

HANZI_COLS = [
    'cp',
    'kMandarin',
    'kCantonese',
    'kSimplifiedVariant',
    'kTraditionalVariant',
]

WORD_COLS = [
    'traditional',
    'simplified',
    'pinyin',
    'pinyin_taiwan',
    'classifiers',
    'alternates',
    'english',
    'german',
    'french',
    'spanish',
]

# JyutDict (zhongwenlearner.com)
# CC-ChEDICC (cc-chedicc.wikispaces.com)
# CFDICT_old (www.chine-informations.com/chinois/open/CFDICT/cfdict_old.zip)

UNIHAN_INFO = {
    'name': 'Unihan',
    'url': 'http://unicode.org/Public/UCD/latest/ucdxml/ucd.unihan.flat.zip',
    'file': 'ucd.unihan.flat.xml',
}

CEDICT_INFO = [
    {
        'name': 'CC-CEDICT',
        'url': 'http://www.mdbg.net/chindict/export/cedict/cedict_1_0_ts_utf-8_mdbg.zip',
        'file': 'cedict_ts.u8',
        'lang': 'english',
        'fields': [
            'traditional',
            'simplified',
            'pinyin',
            'pinyin_taiwan',
            'classifiers',
            'alternates',
            'english',
        ],
    },
    {
        'name': 'HanDeDICT',
        'url': 'http://www.handedict.de/handedict/handedict-20110528.zip',
        'file': 'handedict-20110528/handedict.u8',
        'lang': 'german',
        'fields': [
            'traditional',
            'simplified',
            'pinyin',
            'pinyin_taiwan',
            'classifiers',
            'alternates',
            'german',
        ],
    },
]


def download(info):
    print('Downloading:', info['name'])
    with TemporaryFile() as f:
        f.write(urlopen(info['url']).read())
        ZipFile(f).extract(info['file'], DATA_DIR)


def populate_words():
    conn = connect(DB_PATH)
    c = conn.cursor()
    c.execute('CREATE TABLE cidian (%s)' % ', '.join(WORD_COLS))
    c.execute('CREATE INDEX isimplified ON cidian (simplified)')
    c.execute(
        'CREATE UNIQUE INDEX itraditional ON cidian (traditional, pinyin)'
    )

    for d in CEDICT_INFO:
        with yaspin(
            text='Importing %s' % d['name']
        ).cyan.bold.dots12 as spinner:
            for e in get_cedict_entries(d):
                process_entry(e, c)
            spinner.ok()

    print(
        'Imported {} words'.format(
            c.execute('SELECT count(simplified) FROM cidian').fetchone()[0]
        )
    )

    conn.commit()
    conn.close()


def get_cedict_entries(info):
    with open(join(DATA_DIR, info['file']), encoding='utf-8') as f:
        for line in f:
            pattern = r'^(\S+) (\S+) \[([^\]]+)\] (.+)$'
            result = match(pattern, line)
            if result:
                d = parse_def(result.group(4), info['lang'])
                d['traditional'] = result.group(1)
                d['simplified'] = result.group(2)
                d['pinyin'] = accentuate_pinyin(result.group(3).lower())
                if d[info['lang']]:
                    d[info['lang']] = '\n'.join(
                        accentuate_pinyin_in_definition(s)
                        for s in d[info['lang']]
                        if s
                    )
                yield d


def process_entry(new, cursor):
    try:
        cursor.execute(
            'INSERT INTO cidian ({}) VALUES ({})'.format(
                ','.join(new.keys()), ', '.join(['?'] * len(new))
            ),
            list(new.values()),
        )
    except IntegrityError:
        cursor.execute(
            'SELECT english, german, french, spanish '
            'FROM cidian '
            'WHERE traditional=? AND pinyin=?',
            (new['traditional'], new['pinyin']),
        )
        english, german, french, spanish = cursor.fetchone()
        old = {
            'english': english,
            'german': german,
            'french': french,
            'spanish': spanish,
        }
        for k in old:
            if k in new:
                new[k] = merge_defs(old[k], new[k])
            else:
                new[k] = old[k]
        cursor.execute(
            'UPDATE cidian '
            'SET english=?, german=?, french=?, spanish=? '
            'WHERE traditional=? AND pinyin=?',
            (
                new['english'],
                new['german'],
                new['french'],
                new['spanish'],
                new['traditional'],
                new['pinyin'],
            ),
        )


def parse_def(s, lang):
    d = {lang: []}
    for part in s.split('/'):
        if part.startswith('Taiwan pr.'):
            result = match(r'Taiwan pr. \[(.*?)\]', part)
            if result:
                d['pinyin_taiwan'] = accentuate_pinyin(result.group(1))
        elif part.startswith('CL:'):
            d['classifiers'] = part.replace('CL:', '')
        elif part.startswith('also written'):
            d['alternates'] = part.replace('also written', '')
        else:
            d[lang].append(part)
    return d


def merge_defs(a, b):
    if not a:
        return b
    if not b:
        return a
    return a + '\n' + b


def populate_hanzi():
    conn = connect(DB_PATH)
    c = conn.cursor()
    c.execute('CREATE TABLE hanzi (%s)' % ', '.join(HANZI_COLS))
    c.execute('CREATE UNIQUE INDEX icp ON hanzi (cp)')

    with yaspin(text='Importing Unihan').cyan.bold.dots12 as spinner:
        for e in get_unihan_entries():
            c.execute(
                'INSERT INTO hanzi ({}) VALUES ({})'.format(
                    ','.join(e.keys()), ', '.join(['?'] * len(e))
                ),
                list(e.values()),
            )
        spinner.ok()

    print(
        'Imported {} hanzi'.format(
            c.execute('SELECT COUNT(kMandarin) FROM hanzi').fetchone()[0]
        )
    )

    conn.commit()
    conn.close()


def get_unihan_entries():
    with open(join(DATA_DIR, UNIHAN_INFO['file']), encoding='utf-8') as f:
        data = f.read()

    for char in finditer('<char (.*?)/>', data):
        d = {
            pair.group(1): pair.group(2)
            for pair in finditer('([a-zA-Z0-9_]*)="(.*?)"', char.group(1))
        }
        if d.get('kMandarin') or d.get('kCantonese'):
            for k in list(d):
                if k not in HANZI_COLS:
                    d.pop(k)
                    continue
                if d[k].startswith('U+'):
                    d[k] = ', '.join(
                        [
                            chr(int(codepoint, 16))
                            for codepoint in d[k].replace('U+', '').split()
                        ]
                    )
            d['cp'] = chr(int(d['cp'], 16))
            yield d


def write_license():
    with open(LICENSE_PATH, 'w', encoding='utf-8') as fout:
        fout.write(
            '#########################\n'
            'The %s database was created by aggregating the following sources.\n\n'
            % (DB_PATH)
        )

        fout.write(
            '#########################\n'
            'This database contains an extract of the Unihan database\n\n'
        )

        with open(
            join(DATA_DIR, UNIHAN_INFO['file']), encoding='utf-8'
        ) as fin:
            comments = [
                c.group(1) for c in finditer('<!--(.*?)-->', fin.read())
            ]
        fout.write(''.join(comments) + '\n\n')

        for info in CEDICT_INFO:
            fout.write(
                '#########################\n'
                'This database contains an extract of %s\n\n' % info['name']
            )
            with open(join(DATA_DIR, info['file']), encoding='utf-8') as fin:
                for line in fin:
                    if match('^#', line):
                        fout.write(line)
                fout.write('\n\n')


def cleanup():
    print("Optimizing database size")
    conn = connect(DB_PATH)
    c = conn.cursor()
    c.execute("drop index if exists icp;")
    c.execute("drop index if exists isimplified;")
    c.execute("drop index if exists itraditional;")
    c.execute("vacuum;")
    conn.commit()
    conn.close()


vowel_decorations = [
    {},
    {'a': 'ā', 'e': 'ē', 'i': 'ī', 'o': 'ō', 'u': 'ū', 'ü': 'ǖ', 'v': 'ǖ'},
    {'a': 'á', 'e': 'é', 'i': 'í', 'o': 'ó', 'u': 'ú', 'ü': 'ǘ', 'v': 'ǘ'},
    {'a': 'ǎ', 'e': 'ě', 'i': 'ǐ', 'o': 'ǒ', 'u': 'ǔ', 'ü': 'ǚ', 'v': 'ǚ'},
    {'a': 'à', 'e': 'è', 'i': 'ì', 'o': 'ò', 'u': 'ù', 'ü': 'ǜ', 'v': 'ǜ'},
    {'a': 'a', 'e': 'e', 'i': 'i', 'o': 'o', 'u': 'u', 'ü': 'ü', 'v': 'ü'},
]
accents = 'ɑ̄āĀáɑ́ǎɑ̌ÁǍàɑ̀ÀēĒéÉěĚèÈīĪíÍǐǏìÌōŌóÓǒǑòÒūŪúÚǔǓùÙǖǕǘǗǚǙǜǛ'


def accentuate_pinyin_in_definition(text):
    '''Add accents to pinyin between brackets, as in CCDICT's definitions;
    ex: variant of 不答理[bu4 da1 li3]'''

    def apid_sub(p):
        return accentuate_pinyin(p.group(1))

    text = sub('(\[.*?\])', apid_sub, text, flags=IGNORECASE)
    return text


def accentuate_pinyin(text):
    '''Add accents to pinyin.
    Eg: ni2 becomes ní.
    Eg: ní4 becomes nì. (to make correction easier)
   '''

    def accentuate_pinyin_sub(p):
        pinyin = p.group(1)
        tone = p.group(2)
        if "tone" == pinyin:
            return pinyin + tone
        for v in "aeiouüvAEIOUÜV":
            if pinyin.find(v) > -1:
                try:
                    return sub(
                        v,
                        vowel_decorations[int(tone)][v.lower()],
                        pinyin,
                        count=1,
                    )
                except KeyError as IndexError:
                    pass
        return pinyin

    # correct specific idiosyncracies in CEDICT
    text = text.replace("u:", "v")
    text = text.replace(" r5", " er5")
    # do the actual convertion
    text = sub(
        '([a-z]*[aeiouüÜv' + accents + '][a-zü]*)([1-5])',
        accentuate_pinyin_sub,
        text,
        flags=IGNORECASE,
    )
    return text


def download_all():
    download(UNIHAN_INFO)
    for info in CEDICT_INFO:
        download(info)


def populate_all():
    write_license()
    populate_hanzi()
    populate_words()


def main():
    parser = ArgumentParser()

    parser.add_argument(
        '--delete',
        action='store_true',
        help='delete the database prior to reconstructing it',
    )
    parser.add_argument(
        '--download',
        action='store_true',
        help='download the Unihan and CEDICT dictionaries',
    )
    parser.add_argument(
        '--populate',
        action='store_true',
        help='populate the database from downloaded files',
    )
    parser.add_argument(
        '--cleanup',
        action='store_true',
        help='remove indexes and defragment database',
    )

    args = parser.parse_args()

    if args.delete:
        remove(DB_PATH)
        print('Deleted', DB_PATH)
    if args.download:
        download_all()
    if args.populate:
        populate_all()
    if args.cleanup:
        cleanup()

    if not (args.delete or args.download or args.populate or args.cleanup):
        parser.print_help()
        parser.exit()


if __name__ == '__main__':
    main()
