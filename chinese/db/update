#!/usr/bin/env python3

# Copyright © 2012-2015 Thomas TEMPÉ <thomas.tempe@alysse.org>
# Copyright © 2019 Joseph Lorimer <joseph@lorimer.me>
#
# This file is part of Chinese Support Redux.
#
# Chinese Support Redux is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by the Free
# Software Foundation, either version 3 of the License, or (at your option) any
# later version.
#
# Chinese Support Redux is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
# FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
# more details.
#
# You should have received a copy of the GNU General Public License along with
# Chinese Support Redux.  If not, see <https://www.gnu.org/licenses/>.

from argparse import ArgumentParser
from functools import reduce
from os import remove
from tempfile import TemporaryFile
from urllib.request import urlopen
from zipfile import ZipFile
import re
import sqlite3

dest = "chinese_dict.sqlite"

unihan_fields = [
    "cp",
    "kMandarin",
    "kCantonese",
    "kSimplifiedVariant",
    "kTraditionalVariant",
]

unihan_info = {
    'name': 'Unihan',
    'url': 'http://unicode.org/Public/UCD/latest/ucdxml/ucd.unihan.flat.zip',
    'file': 'ucd.unihan.flat.xml',
}

cedict_fields = [
    "traditional",
    "simplified",
    "pinyin",
    "pinyin_taiwan",
    "classifiers",
    "alternates",
    "english",
    "german",
    "french",
    "spanish",
]
licenses_file = "COPYING.txt"

# JyutDict (zhongwenlearner.com)
# CC-ChEDICC (cc-chedicc.wikispaces.com)
# CFDICT_old (www.chine-informations.com/chinois/open/CFDICT/cfdict_old.zip)

cedicts = [
    {
        'name': 'CC-CEDICT',
        'url': 'http://www.mdbg.net/chindict/export/cedict/cedict_1_0_ts_utf-8_mdbg.zip',
        'file': 'cedict_ts.u8',
        'fields': ' [traditional, simplified, pinyin, pinyin_taiwan, classifiers, alternates, translation, None, None, None ]',
    },
    {
        'name': 'HanDeDICT',
        'url': 'http://www.handedict.de/handedict/handedict-20110528.zip',
        'file': 'handedict-20110528/handedict.u8',
        'fields': ' [traditional, simplified, pinyin, pinyin_taiwan, classifiers, alternates, None, translation, None, None ]',
    },
]

###################################################################
# cedict


def download(info):
    print('Downloading:', info['name'])

    with TemporaryFile() as f:
        f.write(urlopen(info['url']).read())
        ZipFile(f).extract(info['file'], 'data')


def importCedict(cedict, call):
    """Opens the cedict*.zip file containing the dictionary data.
    for each word, calls function 'call', with
    a list of values, matching 'cedict_fields'."""
    fd = open("data/" + cedict["file"], "r", encoding='utf-8')
    licenses = open(licenses_file, "a")
    licenses.write(
        "#########################\nThis database contains an extract of %s\n\n"
        % cedict["name"]
    )

    for word in fd:
        if re.match("^#", word):
            # comment line
            licenses.write(word)
        else:
            items = re.match(r"^(\S+) (\S+) \[(.+?)\] (/.+)", word)
            if items:
                traditional = items.group(1)
                simplified = items.group(2)
                pinyin = accentuate_pinyin(items.group(3).lower())
                rest = items.group(4)
                translation = ""
                pinyin_taiwan = None
                classifiers = None
                alternates = None
                for defs in rest.split("/"):
                    if defs.startswith("Taiwan pr."):
                        try:
                            pinyin_taiwan = re.match(
                                r"Taiwan pr. \[(.*?)\]", defs
                            ).group(1)
                            pinyin_taiwan = accentuate_pinyin(pinyin_taiwan)
                        except:
                            pass
                    elif defs.startswith("CL:"):
                        classifiers = defs[3:]
                    elif defs.startswith("also written"):
                        alternates = defs[13:]
                    else:
                        translation += "\n" + defs
                try:
                    translation = translation[2:-2]
                except:
                    pass
                translation = accentuate_pinyin_in_definition(translation)
                item_as_list = eval(cedict["fields"])
                call(item_as_list)
            else:
                pass  # bogus line
    licenses.write("\n\n")
    licenses.close()


def populateWordsDictionary():
    def fuseDefinition(a, b):
        if a == None:
            return b
        elif b == None:
            return a
        else:
            return a + "\n" + b

    def processEntry(d):
        try:
            c.execute(
                'insert or fail into cidian values (%s)'
                % ("?," * len(d))[:-1],
                d,
            )
        except:
            # If the word already exists, just add its additional translations.
            c.execute(
                "select english, german, french, spanish from cidian where traditional = ? and pinyin= ?",
                (d[0], d[2]),
            )
            english, german, french, spanish = c.fetchone()
            english = fuseDefinition(
                english, d[cedict_fields.index('english')]
            )
            french = fuseDefinition(french, d[cedict_fields.index('french')])
            german = fuseDefinition(german, d[cedict_fields.index('german')])
            spanish = fuseDefinition(
                spanish, d[cedict_fields.index('spanish')]
            )
            c.execute(
                "update cidian set english=?, german=?, french=?, spanish=? where traditional = ? and pinyin= ? ",
                (english, german, french, spanish, d[0], d[2]),
            )

    conn = sqlite3.connect(dest)
    c = conn.cursor()
    try:
        c.execute("drop table cidian;")
    except:
        pass
    c.execute(
        "create table cidian (%s);"
        % reduce(lambda a, b: a + ", " + b, cedict_fields)
    )
    conn.commit()
    c.execute("create index isimplified on cidian ( simplified );")
    #    c.execute("create unique index itraditional on cidian ( traditional);")
    c.execute(
        "create unique index itraditional on cidian ( traditional, pinyin );"
    )
    conn.commit()

    for d in cedicts:
        print("Importing %s" % d["name"])
        importCedict(d, processEntry)

    for a in c.execute("select count(simplified) from cidian;"):
        print("imported %d words" % a)
    conn.commit()

    conn.close()


###################################################################
# Unihan


def importUnihan(call):
    with open('data/ucd.unihan.flat.xml', encoding='utf-8') as f:
        data = f.read()

    licenses = open(licenses_file, "a")
    licenses.write(
        "#########################\nThis database contains an extract of the Unihan databasze\n\n"
    )
    for comment in re.finditer("<!--(.*?)-->", data):
        licenses.write(comment.group(1) + "\n")
    licenses.write("\n\n")
    licenses.close()

    for char in re.finditer("<char (.*?)/>", data):
        c = re.sub(r'([a-zA-Z0-9_]*)="(.*?)"', r'"\1":u"\2",', char.group(1))
        c = "{" + c[:-1] + "}"
        d = unihanFilter(eval(c))
        if d:
            call(d)


def unihanFilter(d):
    """given a dictionary representing one Unihan entry, returns a list of
    fields matching unihan_fields.
    If character does not have mandarin/cantonese pronunciation, return None"""
    values = [None] * len(unihan_fields)
    # Filter out non-chinese characters
    try:
        if d["kMandarin"] == None and d["kCantonese"] == None:
            return None
    except KeyError:
        return None

    # Convert the characters to unicode strings
    d["cp"] = eval("u'\\u%s'" % d["cp"])
    try:
        d["kSimplifiedVariant"] = eval(
            "u'\\u%s'" % d["kSimplifiedVariant"][2:]
        )
    except:
        pass
    try:
        d["kTraditionalVariant"] = eval(
            "u'\\u%s'" % d["kTraditionalVariant"][2:]
        )
    except:
        pass

    # Convert to list
    for i in range(len(unihan_fields)):
        try:
            values[i] = d[unihan_fields[i]]
        except:
            pass
    return values


def populateHanziDB():
    def processEntry(d):
        c.execute('insert into hanzi values (%s)' % ("?," * len(d))[:-1], d)

    print("Importing Hanzi")
    conn = sqlite3.connect(dest)
    c = conn.cursor()
    try:
        c.execute("drop table hanzi;")
    except:
        pass
    c.execute(
        "create table hanzi (%s);"
        % reduce(lambda a, b: a + ", " + b, unihan_fields)
    )
    conn.commit()
    c.execute("create unique index icp on hanzi( cp );")
    conn.commit()

    importUnihan(processEntry)
    for a in c.execute("select count(kMandarin) from hanzi;"):
        print("imported %d characters" % a)
    conn.commit()
    conn.close()


def copyrightNote():
    licenses = open(licenses_file, "w")
    licenses.write(
        "#########################\nThe %s database was created by aggregating the following sources.\n\n"
        % (dest)
    )
    licenses.close()


def cleanup():
    print("Optimizing database size")
    conn = sqlite3.connect(dest)
    c = conn.cursor()
    c.execute("drop index if exists icp;")
    c.execute("drop index if exists isimplified;")
    c.execute("drop index if exists itraditional;")
    c.execute("vacuum;")
    conn.commit()
    conn.close()


###################################################################
# support functions

vowel_decorations = [
    {},
    {'a': 'ā', 'e': 'ē', 'i': 'ī', 'o': 'ō', 'u': 'ū', 'ü': 'ǖ', 'v': 'ǖ'},
    {'a': 'á', 'e': 'é', 'i': 'í', 'o': 'ó', 'u': 'ú', 'ü': 'ǘ', 'v': 'ǘ'},
    {'a': 'ǎ', 'e': 'ě', 'i': 'ǐ', 'o': 'ǒ', 'u': 'ǔ', 'ü': 'ǚ', 'v': 'ǚ'},
    {'a': 'à', 'e': 'è', 'i': 'ì', 'o': 'ò', 'u': 'ù', 'ü': 'ǜ', 'v': 'ǜ'},
    {'a': 'a', 'e': 'e', 'i': 'i', 'o': 'o', 'u': 'u', 'ü': 'ü', 'v': 'ü'},
]
accents = 'ɑ̄āĀáɑ́ǎɑ̌ÁǍàɑ̀ÀēĒéÉěĚèÈīĪíÍǐǏìÌōŌóÓǒǑòÒūŪúÚǔǓùÙǖǕǘǗǚǙǜǛ'


def accentuate_pinyin_in_definition(text):
    '''Add accents to pinyin between brackets, as in CCDICT's definitions;
    ex: variant of 不答理[bu4 da1 li3]'''

    def apid_sub(p):
        return accentuate_pinyin(p.group(1))

    text = re.sub('(\[.*?\])', apid_sub, text, flags=re.I)
    return text


def accentuate_pinyin(text):
    '''Add accents to pinyin.
    Eg: ni2 becomes ní.
    Eg: ní4 becomes nì. (to make correction easier)
   '''

    def accentuate_pinyin_sub(p):
        pinyin = p.group(1)
        tone = p.group(2)
        if "tone" == pinyin:
            return pinyin + tone
        for v in "aeiouüvAEIOUÜV":
            if pinyin.find(v) > -1:
                try:
                    return re.sub(
                        v,
                        vowel_decorations[int(tone)][v.lower()],
                        pinyin,
                        count=1,
                    )
                except KeyError as IndexError:
                    pass
        return pinyin

    # correct specific idiosyncracies in CEDICT
    text = text.replace("u:", "v")
    text = text.replace(" r5", " er5")
    # do the actual convertion
    text = re.sub(
        '([a-z]*[aeiouüÜv' + accents + '][a-zü]*)([1-5])',
        accentuate_pinyin_sub,
        text,
        flags=re.I,
    )
    return text


###################################################################


def download_all():
    download(unihan_info)
    for info in cedicts:
        download(info)


def populate():
    copyrightNote()
    populateHanziDB()
    populateWordsDictionary()


def main():
    parser = ArgumentParser()

    parser.add_argument(
        '--delete',
        action='store_true',
        help='delete the database prior to reconstructing it',
    )
    parser.add_argument(
        '--download',
        action='store_true',
        help='download the UniHan and CEDICT dictionaries',
    )
    parser.add_argument(
        '--populate',
        action='store_true',
        help='populate the database from downloaded files',
    )
    parser.add_argument(
        '--cleanup',
        action='store_true',
        help='remove indexes and defragment database',
    )

    args = parser.parse_args()

    if args.delete:
        print('Deleting', dest)
        remove(dest)
    if args.download:
        download_all()
    if args.populate:
        populate()
    if args.cleanup:
        cleanup()

    if not (args.delete or args.download or args.populate or args.cleanup):
        parser.print_help()
        parser.exit()


if __name__ == '__main__':
    main()
